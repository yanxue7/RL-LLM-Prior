# random seed for batch sampling
seed: 0

# name for this experiment in the local run directory and on wandb
exp_name: "AIF-LLM-Prior_Pretrained_seperate"

# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
batch_size: 4

# the batch size during evaluation and sampling, if enabled
eval_batch_size: 16

# debug mode (disables wandb, model checkpointing, etc.)
debug: false

# the port to use for FSDP
fsdp_port: null

# which dataset(s) to train on; can pass a list like datasets=[hh,shp]
datasets:
- hh

# wandb configuration
wandb:
  enabled: false
  entity: null
  project: "AIF-LLM-Prior"

# to create the local run directory and cache models/datasets,
#   we will try each of these directories in order; if none exist,
#   we will create the last one and use it
local_dirs:
  - /scr-ssd
  - /scr
  - .cache

tensorboard:
  enabled: false

# General Training Config
training_cfg:
  state_dim: 8
  max_length: 500         #thought length
  max_new_tokens:  50
  training_epoch: 2000       #overall training epoch
  gradient_accumulation_steps: 4
  max_grad_norm: 1
  rollout_batch: 25           #total number of environment rollout episode
  buffer_capacity: 2000       # maximum number of MDP pairs
  eval_interval: 20
  eval_epoch: 20

  FSDP: false
  add_state_to_actionP: true    #whether to include state info to actionP's obs

fsdp_cfg:
  log_path: Null
  port: Null


env_cfg:
  name: "FrozenLake-v1"
  map: "4x4"
  is_slippery: false
  max_horizon: 20
  mode: "prompt"     # [symbolic, prompt, string]


model_cfg:
  thought_network:
    path: "/mnt/yansong/YS/pretrained/llama-1b"
    lr: 5e-7
    evaluate_inference_cfg:
      num_beams: 1
      top_k: 20
      top_p: 0.9
      repetition_penalty: 1.1
    rollout_inference_cfg:
      num_beams: 1
      top_k: 50
      top_p: 0.9
      repetition_penalty: 1.2

  prior_llm:
    path: "/mnt/yansong/YS/pretrained/llama-1b"
  internal_model:
    table_model: false   #table recording sa pairs
    transition_hidden_dim: 32
    vae_lr: 5e-4
    transition_lr: 1e-4
  action_network:
    lr: 1e-4
  value_network:
    hidden_dim: 64
    lr: 5e-4


# Pretraining Config
pretraining_cfg:
  flag: false
  iteration: 500
  batch_size: 256
  buffer_capacity: 1000
  vae_epoch: 40
  transition_epoch: 20
  seperate_training: true   #pretrain vae and transition seperately
  turnoff_logvar: false
  std_clip: 0.2
  checkpoint_interval: 200

# Dynamic Training Config
train_dynamic_cfg:
  epoch: 50
  batch_size: 256     #mini batch size
  std_clip: 0.2
  turnoff_logvar: true
  transition_grad_clip: 0.01

# EFE Training Config
train_EFE_cfg:
  epoch: 20
  batch_size: 128
  target_update_interval: 10
  gamma: 0.99
  state_div_coeff: 0.2

# ActionP Training Config
train_ActionP_cfg:
  epoch: 20
  batch_size: 128
  entropy_coeff: 0.5
  grad_clip: 0.5

# SFT Training Config
train_SFT_cfg:
  epoch: 20
  batch_size: 30
  candidate_num: 10    #rejection sampling candidate
  accepted_num: 2
  entropy_coeff: 0.




# whether or not to generate samples during evaluation; disable for FSDP/TensorParallel
#   is recommended, because they are slow
sample_during_eval: true

# how many model samples to generate during evaluation
n_eval_model_samples: 16

# whether to eval at the very beginning of training
do_first_eval: true

# an OmegaConf resolver that returns the local run directory, calling a function in utils.py
local_run_dir: ${get_local_run_dir:${exp_name},${local_dirs}}

# the learning rate
lr: 5e-7

# number of steps to accumulate over for each batch
#   (e.g. if batch_size=4 and gradient_accumulation_steps=2, then we will
#   accumulate gradients over 2 microbatches of size 2)
gradient_accumulation_steps: 1

# the maximum gradient norm to clip to
max_grad_norm: 10.0

# the maximum allowed length for an input (prompt + response)
max_length: 512

# the maximum allowed length for a prompt
max_prompt_length: 256

# the number of epochs to train for; if null, must specify n_examples
n_epochs: 1

# the number of examples to train for; if null, must specify n_epochs
n_examples: null

# the number of examples to evaluate on (and sample from, if sample_during_eval is true)
n_eval_examples: 256

# the trainer class to use (e.g. BasicTrainer, FSDPTrainer, TensorParallelTrainer)
trainer: BasicTrainer

# The optimizer to use; we use RMSprop because it works about as well as Adam and is more memory-efficient
optimizer: RMSprop

# number of linear warmup steps for the learning rate
warmup_steps: 150

# whether or not to use activation/gradient checkpointing
activation_checkpointing: false

# evaluate and save model every eval_every steps
eval_every: 20_000

# prevent wandb from logging more than once per minimum_log_interval_secs
minimum_log_interval_secs: 1.0

defaults:
- _self_
- model: blank_model_fp32 # basic model configuration
- loss: sft # which loss function, either sft or dpo (specify loss.beta if using dpo)
